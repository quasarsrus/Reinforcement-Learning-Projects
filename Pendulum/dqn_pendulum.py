# -*- coding: utf-8 -*-
"""DQN_Pendulum.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RETqwKIPnRoUlCjDRnsOnM-6-zLFK_Yh
"""

from keras.layers import Dense, Activation
from keras.models import Sequential, load_model
from tensorflow.keras.optimizers import Adam
import numpy as np
import pickle
import ankh

import torch
import torch.nn as nn


class Replaybuffer(object):
    def __init__(self, max_size, input_shape, n_actions, discrete = False):
        self.mem_size = max_size
        self.discrete = discrete
        self.mem_counter = 0
        self.state_memory = np.zeros((self.mem_size, input_shape))
        self.newstate_memory = np.zeros((self.mem_size, input_shape))
        dtype = np.int8 if self.discrete else np.float32
        self.action_memory = np.zeros((self.mem_size, n_actions), dtype = dtype)
        self.reward_memory = np.zeros(self.mem_size)
        self.terminal_memory = np.zeros(self.mem_size, dtype = np.float32)

    def storetransition(self, state, action, reward, state_n, done ):
        ind = self.mem_counter % self.mem_size
        self.state_memory[ind] = state
        self.newstate_memory[ind] = state_n
        self.reward_memory[ind] = reward
        self.terminal_memory[ind] = 1 - int(done)
        if self.discrete:
            actions = np.zeros(self.action_memory.shape[1])
            actions[action] = 1.0
            self.action_memory[ind] = actions
        else:
            self.action_memory[ind] = action
        self.mem_counter += 1

    def samplebuffer(self, batch_size):
        max_mem = min(self.mem_counter, self.mem_size)
        batch = np.random.choice(max_mem, batch_size)
        bstate = self.state_memory[batch]
        bstate_new = self.newstate_memory[batch]
        breward = self.reward_memory[batch]
        baction = self.action_memory[batch]
        bterminal = self.terminal_memory[batch]

        return bstate, baction, breward, bstate_new, bterminal

def b_dqn(lrate, n_actions, input_dimens, fc1, fc2):

    model = Sequential([
                    Dense(fc1, input_shape=(input_dimens, ) ),
                    Activation('relu'),
                    Dense(fc2),
                    Activation('relu'),
                    Dense(n_actions)
            ])

    model.compile(optimizer = Adam(lr = lrate), loss = 'mse')

    return model

def torch_dqn(lrate, n_actions, input_dimens, fc1, fc2):

    model = nn.Sequential(nn.Linear(input_dimens, fc1),
                          nn.ReLU(),
                          nn.Linear(fc1, fc2),
                          nn.ReLU(),
                          nn.Linear(fc2, n_actions),
                          )

    loss_function = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lrate)

    return model,loss_function, optimizer


class Agent(object):
    def __init__(self, alpha, gamma, n_actions, epsilon, batch_size,
                 input_dimens, epsilon_decay = 0.999, epsilon_end = 0.01,
                 mem_size = 1000000, fname = 'dqnmodel'):

        self.action_space = [i for i in range(n_actions)]
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_end = epsilon_end
        self.batch_size = batch_size
        self.fname = fname
        self.alpha = alpha

        self.memory = Replaybuffer(mem_size, input_dimens, n_actions, discrete = True)

        #self.q_eval = b_dqn(alpha, n_actions, input_dimens, 256, 256)
        #self.torch_q_eval, self.loss_fcn, self.optim = torch_dqn(alpha, n_actions, input_dimens, 256, 256)

        self.t = NN(None, [input_dimens, 128, 128, 128,  n_actions], ["relu",  "relu" , "relu", "linear"], "mse", initialiser = "he_uniform")

    def remember(self, state, action, reward, new_state, done ):
        self.memory.storetransition(state, action, reward, new_state, done)

    def action_choice(self, state):

        state = state[np.newaxis,:]
        rand = np.random.random()
        if rand<self.epsilon:
            action = np.random.choice(self.action_space)
        else:
            #actions = self.q_eval.predict(state, verbose = False)
            actions = self.t.forward_propagation(state)
            #actions = self.torch_q_eval(torch.from_numpy(state).float())

            #action = torch.argmax(actions)
            action = np.argmax(actions)

        return action

    def learn(self):
        if self.memory.mem_counter < self.batch_size:
            return
        state, action, reward, new_state, done = \
                                        self.memory.samplebuffer(self.batch_size)

        action_values = np.array(self.action_space, dtype = np.int8)

        action_indices = np.dot(action, action_values)

        #q_eval = self.q_eval.predict(state, verbose = False)
        q_eval = self.t.forward_propagation(state);
        #q_eval = self.torch_q_eval(torch.from_numpy(state).float())

        #q_next = self.q_eval.predict(new_state, verbose = False)
        q_next = self.t.forward_propagation(new_state);
        #q_next = self.torch_q_eval(torch.from_numpy(new_state).float())

        q_target = q_eval.copy()
        #q_target = q_eval.clone().detach()

        batch_index = np.arange(self.batch_size, dtype = np.int32)

        q_target[batch_index, action_indices] = reward + \
                                        self.gamma*np.max(q_next, axis = 1)*done

        #q_target[batch_index, action_indices] = torch.tensor(reward + \
                                        #self.gamma*torch.max(q_next, axis = 1)[0].detach().numpy()*done).float()

        #_ = self.q_eval.fit(state, q_target, verbose = False, epochs = 1, steps_per_epoch = 2);

        self.t.train_model(state, q_target, lr = self.alpha, epochs = 15, verbose = 0, \
                            steps_per_epoch = 2, optimiser = "Adam", shuffle = True);

        #loss_torch = self.loss_fcn(q_eval, q_target)
        #self.torch_q_eval.zero_grad()
        #loss_torch.backward()
        #self.optim.step()

        self.epsilon = self.epsilon*self.epsilon_decay if self.epsilon > \
                            self.epsilon_end else self.epsilon_end

    def savemodel(self):
        #self.q_eval.save(self.fname)

        with open('Weights_DQN.pkl', 'wb') as f:
          pickle.dump(self.t.weights, f)

        with open('Bias_DQN.pkl', 'wb') as f:
          pickle.dump(self.t.bias, f)

    def loadmodel(self):
        #self.q_eval = load_model(self.fname)

        with open('Weights_DQN.pkl', 'rb') as f:
          self.t.weights = pickle.load(f)

        with open('Bias_DQN.pkl', 'rb') as f:
          self.t.bias = pickle.load(f)
